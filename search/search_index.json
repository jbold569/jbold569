{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":".md-content__button { display: none; } Technology Leader, Engineer at Heart, Continuous Learner Recent Activity Introducing, DocHive! November 17, 2021 After reusing the code powering this blog across multiple projects, a definite pattern has come to light. At its core, the profile_builder module takes a list of key/value pairs as input, populates a docum... Read More Hello, GitHub Discussions! Goodbye, Disqus... November 02, 2021 In short, I don't like the free version of Disqus. Ads serve a purpose in this world, and they often allow us to enjoy a lot of valuable content at the cost of a portion of our attention. However, I d... Read More Agent Based Logging July 25, 2021 Whether you're working in site reliability, business intelligence, or security, one thing rings true. Logs are king. They are a window into your operations, providing insights into access, change, per... Read More","title":"Home"},{"location":"index.html#technology-leader-engineer-at-heart-continuous-learner","text":"","title":"Technology Leader, Engineer at Heart, Continuous Learner"},{"location":"index.html#recent-activity","text":"","title":"Recent Activity"},{"location":"about.html","text":"Welcome To the point, I love problem solving. I was the kid that was captivated by the wooden lock puzzles at the local book stores. When my parents wouldn't buy me the latest K'Nex set, I all pictures available to me as I attempted to recreate the models with my existing pieces. My passion for creation led me to software and engineering, and continues to fuel my appetite for complex problems. My current path has me leading efforts in defensive/proactive cybersecurity. I got here by riding the \"Big Data\" wave. I've weathered many initiatives designing, building, and managing systems that transport and analyze data at scale. It's my core belief that technology alone can't address the challenges I've encountered, and that time should be taken to invest in those that stand beside you through the endeavor. I've grown to hold many titles. Business owner, leader, architect, friend, husband, handy man, man of many hobbies, and occasionally \"pain in the butt\" (thanks mom). If you've found this site, hopefully you'll learn a little bit about me but more importantly I want you to take my learned experiences back to grow your organizations, your teams, and yourself. \"Humility is the true key to success. Successful people lose their way at times. They often embrace and overindulge from the fruits of success. Humility halts this arrogance and self-indulging trap. Humble people share the credit and wealth, remaining focused and hungry to continue the journey of success.\" - Rick Pitino Resume","title":"About"},{"location":"about.html#welcome","text":"To the point, I love problem solving. I was the kid that was captivated by the wooden lock puzzles at the local book stores. When my parents wouldn't buy me the latest K'Nex set, I all pictures available to me as I attempted to recreate the models with my existing pieces. My passion for creation led me to software and engineering, and continues to fuel my appetite for complex problems. My current path has me leading efforts in defensive/proactive cybersecurity. I got here by riding the \"Big Data\" wave. I've weathered many initiatives designing, building, and managing systems that transport and analyze data at scale. It's my core belief that technology alone can't address the challenges I've encountered, and that time should be taken to invest in those that stand beside you through the endeavor. I've grown to hold many titles. Business owner, leader, architect, friend, husband, handy man, man of many hobbies, and occasionally \"pain in the butt\" (thanks mom). If you've found this site, hopefully you'll learn a little bit about me but more importantly I want you to take my learned experiences back to grow your organizations, your teams, and yourself. \"Humility is the true key to success. Successful people lose their way at times. They often embrace and overindulge from the fruits of success. Humility halts this arrogance and self-indulging trap. Humble people share the credit and wealth, remaining focused and hungry to continue the journey of success.\" - Rick Pitino Resume","title":"Welcome"},{"location":"blog/2021-07-17-automating-docs-as-code.html","text":"Flickr Lisa Brewster var article = document.getElementsByTagName(\"article\")[0]; article.insertBefore(document.getElementById(\"banner\"), article.childNodes[0]); Automating Docs as Code Posted by Jason Bolden on Jul 17, 2021 This may be an unpopular opinion, but software engineers are notoriously bad at maintaining documentation. I don't believe that's any fault of our own. Moving at break-neck speeds to keep up with changing technology while pausing to record the trials and tribulations of the journey takes that much more time away from your Product Owner's tight deadlines. Docs as Code as a methodology is arguably magnitudes more convenient than maintaining a Sharepoint site or Word document, but what if we could make things just a little simpler for the lazy coder? Objective In this article, we're going to explore a handful of methods to make capturing documentation a little more convenient. As a working example, we'll walk through a python module that generates templated blog posts. Github is used to maintain our documentation and actions are leveraged to orchestrate all of our tedious steps. Figure 1 - Conceptual of our 3 phased approach to automation Attention There's a lot of moving pieces in this setup. It's important to note that this write up is focused on the art of the possible and snippets shared are enough for a presentable proof of concept. The Breakdown Assuming you have a fresh repo ready to go, let's start at the top. Automating Inception We're going to use GitHub's functionality as our starting point when creating a new blog, specifically Issues . Just as an issue would typically signal the beginning of a new feature for application development, we're going to treat this as the place where we capture the inspiration for our next blog entry. Since we're trying to cut out as much manual work as possible, let's take advantage of the template functionality GitHub supports for issues. 1 2 3 4 5 6 7 8 9 10 11 12 --- name: Blog Post template about: Automation entry point for creation of new blog posts. title: \"[BLOG]\" labels: documentation.blog assignees: '' --- title: <TITLE> type: <tech, insight, idea> summary: <brief summary of post> Pushing this .md file to your repo under the .github/ISSUE_TEMPLATE directory will result in a pre-filled issue template that presents information we'll need to provide every time we'd like to make a new blog entry. The body is formatted using yaml syntax and the fields will be used as follows: title - The title of the blog entry. type - This will be more apparent later, but depending on the type of blog entry, a different template will be used. summary - This will appear as the first paragraph in the new entry. Figure 2 - Result of our new issue template Important to note is the label used for this template. This will be used in the workflow that gets triggered next. What happens when we create our new issue? Nothing, without a workflow defined. We want this workflow to trigger when a new Issue has been assigned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 name : Orchestrate Issue triggered workflows based on labels on : issues : types : [ assigned ] jobs : blog_issue_assigned : if : ${{github.event.issue.labels[0].name == 'documentation.blog'}} name : Blog Issue Assigned runs-on : ubuntu-latest strategy : matrix : python-version : [ 3.8 ] Now we can use that templated information within a linux environment to take us into phase 2 of our automation workflow. 1 2 3 4 - name : Generate Blog Entry and Clean Up run : | echo \"${{github.event.issue.body}}\" > src/config.yml cat src/config.yml Note The job for building a new blog entry only runs if the issue that triggered the GitHub workflow has a label of documentation.blog . Automating Boilerplate Code Figure 3 - Filled out issue With a brand new issue created, for a moment assume we had to manually create this new blog entry. For our blog, we're using mkdocs . We would need to spend valuable time adding the following to our directory structure: New .md file under the blog directory Type the file name with the correct naming convention Add the new file to the mkdocs.yml file's nav definition 1 2 3 4 5 6 7 8 9 10 . \u251c\u2500\u2500 docs . . . . . . \u2502 \u251c\u2500\u2500 blog \u2502 \u2502 \u251c\u2500\u2500 2021 -07-17-automating-docs-as-code.md \u2502 \u2502 \u2514\u2500\u2500 readme.md \u2502 \u2514\u2500\u2500 index.md \u2514\u2500\u2500 mkdocs.yml 1 2 3 4 5 6 7 nav : - Home : index.md - About : about.md - Blogs : - Content : blog/readme.md - 2021 : - Automating Docs as Code : blog/2021-07-17-automating-docs-as-code.md These steps may appear small and low effort, but over time they accumulate and become the annoyances that cause one to avoid capturing documentation all together. We're going to address this by using some python code. Attention The code referenced in this post are snippets of the profile_builder module used to create it. The entire module is outside of the scope of this discussion, but feel free to explore how the profile_builder works under the hood as an exercise. The logic of the profile_builder is straightforward: Load our config file as a dictionary, builder () Load the appropriate template file (based on the blog type), generate_blog ( ... ) Create the filenames and output directories based on date and config parameters, lines 5-8 1 2 3 4 5 6 7 8 9 # Perform the initial build # Build the .md file and write to blog directory # templates - directory location of \".j2\" files config = builder () config [ 'publish_date' ] = date . today () template_file = f \" { templates } /blog-post- { config [ 'type' ] } .md.j2\" blog_filename = f \" { config [ 'publish_date' ] : %Y-%m-%d } - { slugify ( config [ 'title' ]) } .md\" dest_file = f \" { output_dir } / { blog_filename } \" generate_blog ( template_file , config , dest_file ) Note Notice that we've parameterized the template filename using the type variable from the issue config. 1 template_file = f \" { templates } /blog-post- { config [ 'type' ] } .md.j2\" Jinja is a great template engine for programmatically creating documents. Because blogs are pretty much rinse and repeat of the same formatting, why should we spend the time typing it out every time (or even copy pasting for that matter)? Jinja allows us to use the dict elements in our config variable to populate the {{...}} expression fields of our template. Once generated, our new entry is written to the dest_file directory. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 <div id=\"banner\" class=\"page-image\"> <img src=\"../img/ {{ publish_date.strftime ( '%Y-%m-%d' ) }} -blog-banner.drawio.svg\" alt=\"\"> <div class=\"page-image-caption\"> <p> <a href=\"\">AUTHOR</a> </p> </div> </div> <!-- Reposition the banner image on top --> <script> var article = document.getElementsByTagName(\"article\")[0]; article.insertBefore(document.getElementById(\"banner\"), article.childNodes[0]); </script> # {{ title }} {{ publish_date.strftime ( '%Y %b %d' ) }} by [Jason Bolden](../about.md) {{ summary }} ## Objective ## Steps ## Conclusion ## References Lastly, we need to update mkdocs.yml file to include our new 2021-07-17-automating-docs-as-code.md file. 1 2 3 4 5 6 7 # get the relative path from within the docs directory p = Path ( dest_file ) entry = Path ( * p . parts [ 2 :]) # Update the mkdocs.yml file nav with the new structure mkdocs_config = load_config ( config_file = mkdocs_file ) update_blog ( mkdocs_config , config [ 'publish_date' ], { config [ 'title' ]: str ( entry )}) Automating Dev Prep We're almost done. The only thing remaining is to stitch our profile_builder into our issues_automation workflow and create our development branch. Again, we could create a new branch manually and run the profile_builder module from the command line on a our local, but where's the fun in that? 1 2 3 - name : Create new Blog Branch run : | git checkout -b ${{BRANCH_NAME}} 1 2 3 4 5 6 7 8 9 10 11 12 13 - name : Generate Blog Entry and Clean Up run : | echo \"${{github.event.issue.body}}\" > src/config.yml cat src/config.yml cd src python -m profile_builder blog -t ./templates -o ../docs/blog -c ./config.yml -v rm config.yml cd .. git config user.name github-actions git config user.email github-actions@github.com git add . git commit -m \"generated ${{BRANCH_NAME}}\" git push --set-upstream origin ${{BRANCH_NAME}} This code snippet builds on our GitHub Workflow step from phase 1 . All we're doing is scripting out the actions previously described in bash. To kick the whole thing off, simply create a new Blog issue, fill in the details, assign it to yourself, and switch to the newly created blog branch. Figure 4 - Completed Action workflow following issue assignment Conclusion At first glance, this may seem like overkill for a simple blog. A Wordpress or Squarespace site would be much easier to put together. So instead, let's think about a repo that holds engineering artifacts for the application your team supports. Or how about a knowledge base of training material for your team of developers. Incident response playbooks for your Security Operations Center, IT Help Desk procedures, etc. This workflow allows for someone to suggest an addition to the team's documentation, and automate a lot of the repetitive actions that demotivates the individual from creating the documentation in the first place. In addition to the bonus of treating the docs as managed source code, the GitHub repo facilitates a collaborative environment so documentation is less likely to be created in a vacuum without peer review. References Docs Like Code Jinja MkDocs GitHub Actions GitHub Issues YAML","title":"Automating Docs as Code"},{"location":"blog/2021-07-17-automating-docs-as-code.html#automating-docs-as-code","text":"Posted by Jason Bolden on Jul 17, 2021 This may be an unpopular opinion, but software engineers are notoriously bad at maintaining documentation. I don't believe that's any fault of our own. Moving at break-neck speeds to keep up with changing technology while pausing to record the trials and tribulations of the journey takes that much more time away from your Product Owner's tight deadlines. Docs as Code as a methodology is arguably magnitudes more convenient than maintaining a Sharepoint site or Word document, but what if we could make things just a little simpler for the lazy coder?","title":"Automating Docs as Code"},{"location":"blog/2021-07-17-automating-docs-as-code.html#objective","text":"In this article, we're going to explore a handful of methods to make capturing documentation a little more convenient. As a working example, we'll walk through a python module that generates templated blog posts. Github is used to maintain our documentation and actions are leveraged to orchestrate all of our tedious steps. Figure 1 - Conceptual of our 3 phased approach to automation Attention There's a lot of moving pieces in this setup. It's important to note that this write up is focused on the art of the possible and snippets shared are enough for a presentable proof of concept.","title":"Objective"},{"location":"blog/2021-07-17-automating-docs-as-code.html#the-breakdown","text":"Assuming you have a fresh repo ready to go, let's start at the top.","title":"The Breakdown"},{"location":"blog/2021-07-17-automating-docs-as-code.html#automating-inception","text":"We're going to use GitHub's functionality as our starting point when creating a new blog, specifically Issues . Just as an issue would typically signal the beginning of a new feature for application development, we're going to treat this as the place where we capture the inspiration for our next blog entry. Since we're trying to cut out as much manual work as possible, let's take advantage of the template functionality GitHub supports for issues. 1 2 3 4 5 6 7 8 9 10 11 12 --- name: Blog Post template about: Automation entry point for creation of new blog posts. title: \"[BLOG]\" labels: documentation.blog assignees: '' --- title: <TITLE> type: <tech, insight, idea> summary: <brief summary of post> Pushing this .md file to your repo under the .github/ISSUE_TEMPLATE directory will result in a pre-filled issue template that presents information we'll need to provide every time we'd like to make a new blog entry. The body is formatted using yaml syntax and the fields will be used as follows: title - The title of the blog entry. type - This will be more apparent later, but depending on the type of blog entry, a different template will be used. summary - This will appear as the first paragraph in the new entry. Figure 2 - Result of our new issue template Important to note is the label used for this template. This will be used in the workflow that gets triggered next. What happens when we create our new issue? Nothing, without a workflow defined. We want this workflow to trigger when a new Issue has been assigned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 name : Orchestrate Issue triggered workflows based on labels on : issues : types : [ assigned ] jobs : blog_issue_assigned : if : ${{github.event.issue.labels[0].name == 'documentation.blog'}} name : Blog Issue Assigned runs-on : ubuntu-latest strategy : matrix : python-version : [ 3.8 ] Now we can use that templated information within a linux environment to take us into phase 2 of our automation workflow. 1 2 3 4 - name : Generate Blog Entry and Clean Up run : | echo \"${{github.event.issue.body}}\" > src/config.yml cat src/config.yml Note The job for building a new blog entry only runs if the issue that triggered the GitHub workflow has a label of documentation.blog .","title":"Automating Inception"},{"location":"blog/2021-07-17-automating-docs-as-code.html#automating-boilerplate-code","text":"Figure 3 - Filled out issue With a brand new issue created, for a moment assume we had to manually create this new blog entry. For our blog, we're using mkdocs . We would need to spend valuable time adding the following to our directory structure: New .md file under the blog directory Type the file name with the correct naming convention Add the new file to the mkdocs.yml file's nav definition 1 2 3 4 5 6 7 8 9 10 . \u251c\u2500\u2500 docs . . . . . . \u2502 \u251c\u2500\u2500 blog \u2502 \u2502 \u251c\u2500\u2500 2021 -07-17-automating-docs-as-code.md \u2502 \u2502 \u2514\u2500\u2500 readme.md \u2502 \u2514\u2500\u2500 index.md \u2514\u2500\u2500 mkdocs.yml 1 2 3 4 5 6 7 nav : - Home : index.md - About : about.md - Blogs : - Content : blog/readme.md - 2021 : - Automating Docs as Code : blog/2021-07-17-automating-docs-as-code.md These steps may appear small and low effort, but over time they accumulate and become the annoyances that cause one to avoid capturing documentation all together. We're going to address this by using some python code. Attention The code referenced in this post are snippets of the profile_builder module used to create it. The entire module is outside of the scope of this discussion, but feel free to explore how the profile_builder works under the hood as an exercise. The logic of the profile_builder is straightforward: Load our config file as a dictionary, builder () Load the appropriate template file (based on the blog type), generate_blog ( ... ) Create the filenames and output directories based on date and config parameters, lines 5-8 1 2 3 4 5 6 7 8 9 # Perform the initial build # Build the .md file and write to blog directory # templates - directory location of \".j2\" files config = builder () config [ 'publish_date' ] = date . today () template_file = f \" { templates } /blog-post- { config [ 'type' ] } .md.j2\" blog_filename = f \" { config [ 'publish_date' ] : %Y-%m-%d } - { slugify ( config [ 'title' ]) } .md\" dest_file = f \" { output_dir } / { blog_filename } \" generate_blog ( template_file , config , dest_file ) Note Notice that we've parameterized the template filename using the type variable from the issue config. 1 template_file = f \" { templates } /blog-post- { config [ 'type' ] } .md.j2\" Jinja is a great template engine for programmatically creating documents. Because blogs are pretty much rinse and repeat of the same formatting, why should we spend the time typing it out every time (or even copy pasting for that matter)? Jinja allows us to use the dict elements in our config variable to populate the {{...}} expression fields of our template. Once generated, our new entry is written to the dest_file directory. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 <div id=\"banner\" class=\"page-image\"> <img src=\"../img/ {{ publish_date.strftime ( '%Y-%m-%d' ) }} -blog-banner.drawio.svg\" alt=\"\"> <div class=\"page-image-caption\"> <p> <a href=\"\">AUTHOR</a> </p> </div> </div> <!-- Reposition the banner image on top --> <script> var article = document.getElementsByTagName(\"article\")[0]; article.insertBefore(document.getElementById(\"banner\"), article.childNodes[0]); </script> # {{ title }} {{ publish_date.strftime ( '%Y %b %d' ) }} by [Jason Bolden](../about.md) {{ summary }} ## Objective ## Steps ## Conclusion ## References Lastly, we need to update mkdocs.yml file to include our new 2021-07-17-automating-docs-as-code.md file. 1 2 3 4 5 6 7 # get the relative path from within the docs directory p = Path ( dest_file ) entry = Path ( * p . parts [ 2 :]) # Update the mkdocs.yml file nav with the new structure mkdocs_config = load_config ( config_file = mkdocs_file ) update_blog ( mkdocs_config , config [ 'publish_date' ], { config [ 'title' ]: str ( entry )})","title":"Automating Boilerplate Code"},{"location":"blog/2021-07-17-automating-docs-as-code.html#automating-dev-prep","text":"We're almost done. The only thing remaining is to stitch our profile_builder into our issues_automation workflow and create our development branch. Again, we could create a new branch manually and run the profile_builder module from the command line on a our local, but where's the fun in that? 1 2 3 - name : Create new Blog Branch run : | git checkout -b ${{BRANCH_NAME}} 1 2 3 4 5 6 7 8 9 10 11 12 13 - name : Generate Blog Entry and Clean Up run : | echo \"${{github.event.issue.body}}\" > src/config.yml cat src/config.yml cd src python -m profile_builder blog -t ./templates -o ../docs/blog -c ./config.yml -v rm config.yml cd .. git config user.name github-actions git config user.email github-actions@github.com git add . git commit -m \"generated ${{BRANCH_NAME}}\" git push --set-upstream origin ${{BRANCH_NAME}} This code snippet builds on our GitHub Workflow step from phase 1 . All we're doing is scripting out the actions previously described in bash. To kick the whole thing off, simply create a new Blog issue, fill in the details, assign it to yourself, and switch to the newly created blog branch. Figure 4 - Completed Action workflow following issue assignment","title":"Automating Dev Prep"},{"location":"blog/2021-07-17-automating-docs-as-code.html#conclusion","text":"At first glance, this may seem like overkill for a simple blog. A Wordpress or Squarespace site would be much easier to put together. So instead, let's think about a repo that holds engineering artifacts for the application your team supports. Or how about a knowledge base of training material for your team of developers. Incident response playbooks for your Security Operations Center, IT Help Desk procedures, etc. This workflow allows for someone to suggest an addition to the team's documentation, and automate a lot of the repetitive actions that demotivates the individual from creating the documentation in the first place. In addition to the bonus of treating the docs as managed source code, the GitHub repo facilitates a collaborative environment so documentation is less likely to be created in a vacuum without peer review.","title":"Conclusion"},{"location":"blog/2021-07-17-automating-docs-as-code.html#references","text":"Docs Like Code Jinja MkDocs GitHub Actions GitHub Issues YAML","title":"References"},{"location":"blog/2021-07-25-agent-based-logging.html","text":"El Taller del Bit var article = document.getElementsByTagName(\"article\")[0]; article.insertBefore(document.getElementById(\"banner\"), article.childNodes[0]); Agent Based Logging Posted by Jason Bolden on Jul 25, 2021 Whether you're working in site reliability, business intelligence, or security, one thing rings true. Logs are king. They are a window into your operations, providing insights into access, change, performance, who, what, when, where, and why. However, just as logs can be invaluable they can just as easily be burdensome and costly. Poor log hygiene plagues many organizations. While many business functions benefit from logs, they don't benefit from all logs and the excess translates directly to cost. In this post we'll explore some options for logging instrumentation that aid in filtering, routing, and maintaining the flow of logs at the host level. Objective In this post we'll explore 3 popular logging agents to better understand the pros and cons of each solution for different scenarios. The contenders are: Created in 2011, Fluentd is one of the most popular logging agents in the cloud computing space. The project was open-sourced in the same year, and Google supports their own flavor as their standard cloud logging agent. In 2014, Fluentbit was created as a lighter weight agent for IoT workloads. NXLog was created back in 2009 as an alternative to msyslog. Originally a closed source project, NXLog Community Edition was open-sourced in 2011 and has been free since. NXLog has a reputation in the cybersecurity space as a windows event log collector. SANS' SEC555 course references NXLog as a reliable free option for gaining visibility into windows and system logging. Apache NiFi began as a project created by the NSA. It was later introduced to the Apache Software Foundation, and subsequently commercialized by Hortonworks (now Cloudera ). The tool is a robust Data Flow Controller with the goal of making the automation and management of ETL processes simpler and more maintainable. MiNiFi is a sub-project that borrows the fundamental concepts defined by NiFi, but packages them in a smaller form factor for deployment to endpoints and IoT devices. We'll run through a basic deployment to a Windows desktop to demonstrate local setup and Kubernetes for cloud. In my personal experience, I've leveraged NiFi quite extensively. To be as objective as possibly in our evaluation, we'll measure each tool according to the following criteria: Documentation - This will be judged on completeness, number of examples, and searchability Ease of Use - How short is the time to get up and running, does the agent support monitoring, is it easy to maintain? Cloud Readiness - What cloud providers are supported? Does the documentation describe cloud installation? Architecture - How well is the agent designed, how resilient is it to operational disruption, can the base functionality be easily extended? For the sake of time, evaluating ease of use and documentation will be based on my ability to set up the agents after 30 minutes of reading the documentation. The following Figures depict our lab setup. Figure 1 - Conceptual Local Setup Figure 2 - Conceptual Cloud Setup All configurations and scripts for this post can be found in this GitHub repo . Tip When investigating multiple tools that solve the same problem, it's always a good idea to use time boxes. While 30 minutes is a bit extreme, it's not uncommon to dedicate a day to hacking out an MVP to assess the feasibility of using one solution over another. The Breakdown Logging agents typically follow a 3 part architecture. Source , defines how the agent interfaces with the log producing system. Channel , a transport layer. Data are stored, transformed, and/or filtered in this layer. Lastly, Sink defines the interface with the destination of the data. As we describe the architecture of the three Solutions, we'll tie their components back to these three constructs for consistency and ease of comparison. Figure 3 - Conceptual 3 part architecture for logging agents Note The astute would recognize this terminology from the Apache Flume project. This was the first logging agent I used in a production setting. Many projects have sprung up since, but their underlying architecture remains more or less the same. We'll also be using this short python script to produce our applications logs in each test. 1 2 3 4 5 6 7 8 from datetime import datetime import time , socket , sys while True : with open ( sys . argv [ 1 ], 'a' ) as f : f . write ( f ' { datetime . now () : %Y-%m-%dT%H:%M:%S } { socket . gethostname () } Hello, World! \\n ' ) f . close () time . sleep ( 2 ) This script is used in both the Local and Cloud setups. The following is the Dockerfile definition for our chatty-app. 1 2 3 4 5 6 7 FROM python:3.8-slim-buster WORKDIR /app COPY log-gen.py log-gen.py CMD [ \"python3\" , \"log-gen.py\" , \"/var/log/log-file.log\" ] Minikube and Docker should be installed on your machine if you intend to follow along. chatty-app must be built with the minikube environment variables set beforehand. 1 2 3 4 5 minikube start eval $( minikube docker-env ) cd logging-agent-eval/cloud docker build --tag chatty-app . docker images Fluentbit Fluentbit doesn't deviate much from our established formula. Configuration of the agent boasts a simplistic format and schema , utilizing Sections and indented key/value paired Entries Figure 4 - Fluentbit Data Pipeline Model 30 Minute Sprint After 30 minutes of combing the documentation, I felt pretty comfortable to start hacking away at an MVP . The docs are organized using GitBook , making it fairly painless to navigate. If I had to be nit-picky, there's quite a few grammatical errors throughout. It's not unreadable, but it does throw one off a bit. Concerning the bells and whistles, security is supported through the use of TLS. All outputs that require network I/O support options for TLS configuration. Agents are resilient through the use of buffering , enabling persistance to disk. Agent health can be monitored via API calls when configured. Alternatively, one could use the Prometheus Exporter output plugin to route monitoring metrics directly to a prometheus server. configuration . Fluentbit also has a handy visualizer to aid in flow development. This can be very useful when troubleshooting complex dataflows. Attention Take care with buffering strategies on cloud workloads. Due to the ephemeral nature of container resources, one should ensure the Fluentbit storage paths point to persistent volumes in your K8S deployment. Local Setup After downloading the ZIP archive for windows, create two new *.conf files to save the following settings: fluent-bit.conf defines our data flow. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [SERVICE] flush 5 daemon Off log_level info parsers_file parsers.conf [INPUT] name tail tag log_file path ../log-file.log parser custom [INPUT] name winlog tag win_log channels Security Interval_Sec 1 db winlog.sqlite [OUTPUT] name tcp match * host 127.0.0.1 port 8514 format json_lines parsers.conf holds our parsing definitions. In our case, we only define one parser to handle logs from the log-gen.py script. 1 2 3 4 5 6 [PARSER] Name custom Format regex Regex ^(?<time>[^ ]*) (?<host>[^ ]*) (?<message>.*)?$ Time_Key time Time_Format %Y-%m-%dT%H:%M:%S Attention Ensure that the tail INPUT path parameter matches the path used by the log-gen.py script. With our config files in hand, we can run the following commands to start our test. Run the logger script. 1 \u276f python log-gen . py log -file . log In another terminal, with administrative privileges. 1 2 \u276f cd td-agent-bit -* \u276f ./ bin / fluent-bit . exe -c ./ conf / fluent-bit . conf Lastly, on the receiving end, we'll start our TCP listener. 1 2 3 4 5 6 7 # From WSL \u276f nc -l 127 .0.0.1 8514 { \"date\" :1627405959.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405961.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405963.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405965.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405967.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } Note I've left out the windows event logs from the samples above to minimize sharing of sensitive information. Cloud Setup More time was spent configuring the chatty-app container and troubleshooting minikube than actually configuring the fluent-bit container. The config files for this setup are listed below: fluent-bit.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [SERVICE] flush 5 daemon Off log_level info parsers_file parsers.conf [INPUT] name tail tag log_file path /var/log/log-file.log parser custom [OUTPUT] name stdout match * parser.conf is the same as before. cloud-logging-fluentbit.yml is our pod configuration for K8S 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : v1 kind : Pod metadata : name : test-pod spec : containers : - name : chatty-app image : chatty-app imagePullPolicy : Never volumeMounts : - name : varlog mountPath : /var/log - name : fluent-bit image : fluent/fluent-bit volumeMounts : - name : varlog mountPath : /var/log - name : config mountPath : /fluent-bit/etc volumes : - name : varlog emptyDir : {} - name : config configMap : name : fluentbit-configmap The fluentbit container config mounts our user defined configmap to the expected directory. To deploy run the following. 1 2 3 \u276f kubectl create configmap fluentbit-configmap --from-file = fluent-bit.conf --from-file = parsers.conf \u276f kubectl apply -f ./cloud-logging-fluentbit.yml \u276f minikube dashboard & Navigate to the provisioned pod and view the logs of the fluent-bit container. Figure 5 - Log output from the fluent-bit container in the K8S UI Note Fluentd and Kubernetes both document using Fluentbit agents in a daemonset configuration rather than a sidecar. For consistency in our testing we will not follow that recommendation; however, the sidecar configuration is still a recommended logging pattern documented by Kubernetes. NXLog NXLog may appear simple conceptually ; however, each component is very extensible. Figure 6 - NXLog Conceptual Architecture 30 Minute Sprint NXLog is definitely a very mature project. The documentation is dense and thorough. When I stumbled on their expression language page, I knew I was headed into power user territory. NXLog Routes process event records , collections of fields . The configuration file is made up of 2 main constructs, Directives and Modules . There are 4 types of Modules . Inputs consume event records from source systems and optionally parse incoming records. Processors provide functionality for transforming, buffering, and/or filtering event records. Outputs emit event records to a destination system. Lastly, Extensions provide extended functionality to the NXLog language. Directives are parameters that define the various components of the NXLog configuration. Several examples of buffering strategies are documented. Monitoring functionality is lackluster . Documentation only mentions OS specific methods for ensuring NXLog runs as a service; however, collection of health metrics is not mentioned. There are several useful features that are only available in the enterprise version of the agent. Their feature comparison section goes into more depth. In all, solid knowledge base backing the project. Almost overwhelming, but not so much so that we can't get an MVP running quickly. Security is supported via Input and Output modules that handle SSL/TLS configurations. Local Setup After downloading the installer and completing installation , create one new *.conf files to save the following settings: nxlog.conf defines our data flow. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 <Extension _json> Module xm_json </Extension> <Input winlog> Module im_wseventing Exec to_json(); <QueryXML> <QueryList> <Query Id=\"0\" Path=\"Application\"> <Select Path=\"System\">*</Select> </Query> </QueryList> </QueryXML> </Input> define EVENT_REGEX /(?x)^(\\d+-\\d+-\\d+T\\d+:\\d+:\\d+)\\s+(\\S+)\\s+(.*)/ <Input file> Module im_file File \"C:\\\\logs\\\\log-file.log\" <Exec> if $raw_event =~ %EVENT_REGEX% { $EventTime = strptime($1,'%Y-%m-%dT%T'); $Host = $2; $Message = $3; to_json(); } else drop(); </Exec> </Input> <Output tcp> Module om_tcp Host 127.0.0.1 Port 8514 </Output> <Route winlog_to_tcp> Path winlog => tcp </Route> <Route file_to_tcp> Path file => tcp </Route> Attention Ensure that the File directive matches the path used by the log-gen.py script. Testing procedures are as follows. Run the logger script. 1 \u276f python log-gen . py log -file . log In another terminal, with administrative privileges. 1 2 \u276f cd $NXLOG_PATH \u276f ./ nxlog . exe -c ./ nxlog . conf Lastly, on the receiving end, we'll start our TCP listener. 1 2 3 4 5 6 # From WSL \u276f nc -l 127 .0.0.1 8514 { \"EventReceivedTime\" : \"2021-07-27 22:34:56\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:34:56\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } { \"EventReceivedTime\" : \"2021-07-27 22:34:58\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:34:58\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } { \"EventReceivedTime\" : \"2021-07-27 22:35:00\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:35:00\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } { \"EventReceivedTime\" : \"2021-07-27 22:35:02\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:35:02\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } Attention NXLog for Windows is an .msi installation. For my setup, I added the path to the nxlog.exe to my system path for ease of use. $NXLOG_PATH is not defined by default. Note Again, I've left out the windows event logs from the samples above to minimize sharing of sensitive information. Cloud Setup The K8S config for NXLog is a bit different than for Fluentbit. Instead of writing the logs to stdout , we've configured NXLog to write them to the agent's internal log file. NXLog does not support writing to stdout . NXLog documentation provides instructions to build the docker image for the agent locally; however, it can be pull remotely from DockerHub. The config files for this setup are listed below: nxlog.conf - Logs are output to the om_null module, and we use the log_info() function to capture them instead 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 User nxlog Group nxlog LogFile /var/log/nxlog.log LogLevel INFO <Extension _json> Module xm_json </Extension> define EVENT_REGEX /(?x)^(\\d+-\\d+-\\d+T\\d+:\\d+:\\d+)\\s+(\\S+)\\s+(.*)/ <Input file> Module im_file File \"/var/log/log-file.log\" <Exec> if $raw_event =~ %EVENT_REGEX% { $EventTime = strptime($1,'%Y-%m-%dT%T'); $Host = $2; $Message = $3; log_info(to_json()); } else drop(); </Exec> </Input> <Output null> Module om_null </Output> <Route file_to_tcp> Path file => null </Route> cloud-logging-nxlog.yml is our pod configuration for K8S 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion : v1 kind : Pod metadata : name : test-pod spec : containers : - name : chatty-app image : chatty-app imagePullPolicy : Never volumeMounts : - name : varlog mountPath : /var/log - name : nxlog-ce image : nxlog/nxlog-ce args : [ \"-c\" , \"/etc/nxlog/nxlog.conf\" ] volumeMounts : - name : varlog mountPath : /var/log - name : config mountPath : /etc/nxlog volumes : - name : varlog emptyDir : {} - name : config configMap : name : nxlog-configmap Like before, the NXLog container config mounts our user defined configmap to the expected directory. In this case, the path to the config is passed as a container argument. To deploy run the following. 1 2 3 \u276f kubectl create configmap nxlog-configmap --from-file = nxlog.conf \u276f kubectl apply -f ./cloud-logging-nxlog.yml \u276f minikube dashboard & Navigate to the provisioned pod and start the terminal for the nxlog-ce container. Execute tail -f /var/log/nxlog.log to see the agent logs. Figure 7 - Log output from the nxlog-ce container in the K8S UI Note NXLog documents both Daemonset and Sidecar configurations for logging. MiNiFi Coined the Swiss Army Knife of Dataflows, there's almost nothing you can't do with Apache NiFi. Eat your heart out on the documentation . Figure 8 - MiNiFi conceptual architecture 30 Minute Sprint NiFi/MiNiFi uses Processors to create, transform, and transmit Flowfiles . That's pretty much it. Full disclosure, I have many years of experience working with NiFi. I've deployed and maintained NiFi clusters in multiple data centers, developed configurations for MiNiFi in the cloud, and deployed agents to thousands of self service machines. I'm no stranger to the tool, so the 30 minutes time cap doesn't really apply here. That said, my experience with this tool did not help me as much as it should have for our two test cases. For the sake of time and, frankly, to limit the scope of this post, I had to cut the testing of the MiNiFi agent short as I was going way too far in the weeds trying to get the configuration working. While NiFi and MiNiFi sport all the features you'd want in an enterprise setting ( security , resiliency , monitoring , flexibility, extensibility, etc.), the tool is massive and requires a lot of overhead to utilize effectively. Not to add insult to injury, but MiNiFi has two version, C++ and Java. The Java agent, for all intensive purposes, is a headless version of NiFi with fewer core packages included. It's still a hefty agent, but you have 100% feature parity with the server version. The C++ agent is much lighter, however not all functionality is supported. Local Setup MiNiFi agents are configured using as single yaml ; however, it is not recommended that one writes a configuration from a text editor (you'll soon see why). The Quickstart walks through how to create a dataflow from the NiFi UI, save it as a template, export, and convert to the .yml config accepted by MiNiFi. Figure 9 - NiFi UI depicting local dataflow setup After conversion, the flow above becomes... conf.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 MiNiFi Config Version : 3 Flow Controller : name : local-setup comment : '' Core Properties : flow controller graceful shutdown period : 10 sec flow service write delay interval : 500 ms administrative yield duration : 30 sec bored yield duration : 10 millis max concurrent threads : 1 variable registry properties : '' FlowFile Repository : partitions : 256 checkpoint interval : 2 mins always sync : false Swap : threshold : 20000 in period : 5 sec in threads : 1 out period : 5 sec out threads : 4 Content Repository : implementation : org.apache.nifi.controller.repository.FileSystemRepository content claim max appendable size : 10 MB content claim max flow files : 100 content repository archive enabled : false content repository archive max retention period : 12 hours content repository archive max usage percentage : 50% always sync : false Provenance Repository : provenance rollover time : 1 min implementation : org.apache.nifi.provenance.WriteAheadProvenanceRepository provenance index shard size : 500 MB provenance max storage size : 1 GB provenance max storage time : 24 hours provenance buffer size : 10000 Component Status Repository : buffer size : 1440 snapshot frequency : 1 min Security Properties : keystore : '' keystore type : '' keystore password : '' key password : '' truststore : '' truststore type : '' truststore password : '' ssl protocol : '' Sensitive Props : key : gAHRSHkEd0lN7Vy1SW20sgZhsNjyGSWx algorithm : PBEWITHMD5AND256BITAES-CBC-OPENSSL provider : BC Processors : - id : 4933996e-f262-3afa-0000-000000000000 name : ConsumeWindowsEventLog class : org.apache.nifi.processors.windows.event.log.ConsumeWindowsEventLog max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : [] Properties : channel : Application inactiveDurationToReconnect : 10 mins maxBuffer : '1048576' maxQueue : '1024' query : |- <QueryList> <Query Id=\"0\" Path=\"Application\"> <Select Path=\"Application\">*</Select> </Query> </QueryList> - id : ffaea6de-2e7c-3964-0000-000000000000 name : PutElasticsearch class : org.apache.nifi.processors.elasticsearch.PutElasticsearch max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : [] Properties : Batch Size : '100' Character Set : UTF-8 Cluster Name : elasticsearch ElasticSearch Hosts : ElasticSearch Ping Timeout : 5s Identifier Attribute : Index : Index Operation : index Password : SSL Context Service : Sampler Interval : 5s Shield Plugin Filename : Type : Username : - id : a6cc363f-e088-3e94-0000-000000000000 name : PutTCP class : org.apache.nifi.processors.standard.PutTCP max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : - failure - success Properties : Character Set : UTF-8 Connection Per FlowFile : 'false' Hostname : localhost Idle Connection Expiration : 5 seconds Max Size of Socket Send Buffer : 1 MB Outgoing Message Delimiter : Port : '8514' SSL Context Service : Timeout : 10 seconds - id : 3ece5573-f82e-3f5f-0000-000000000000 name : TailFile class : org.apache.nifi.processors.standard.TailFile max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : [] Properties : File Location : Local File to Tail : C:\\\\logs\\\\log-file.log Initial Start Position : Beginning of File Post-Rollover Tail Period : 0 sec Rolling Filename Pattern : reread-on-nul : 'false' tail-base-directory : tail-mode : Single file tailfile-lookup-frequency : 10 minutes tailfile-maximum-age : 24 hours tailfile-recursive-lookup : 'false' Controller Services : [] Process Groups : [] Input Ports : [] Output Ports : [] Funnels : [] Connections : - id : e7bee299-ce60-3591-0000-000000000000 name : ConsumeWindowsEventLog/success/PutTCP source id : 4933996e-f262-3afa-0000-000000000000 source relationship names : - success destination id : a6cc363f-e088-3e94-0000-000000000000 max work queue size : 10000 max work queue data size : 1 GB flowfile expiration : 0 sec queue prioritizer class : '' - id : b1d11f20-c37d-365e-0000-000000000000 name : TailFile/success/PutTCP source id : 3ece5573-f82e-3f5f-0000-000000000000 source relationship names : - success destination id : a6cc363f-e088-3e94-0000-000000000000 max work queue size : 10000 max work queue data size : 1 GB flowfile expiration : 0 sec queue prioritizer class : '' Remote Process Groups : [] NiFi Properties Overrides : {} Testing procedures are as follows. We're not going into great detail on this one. Run the logger script. 1 \u276f python log-gen . py log -file . log In another terminal, ensure that the config.yml file is in the conf directory for MiNiFi. Start the agent. 1 \u276f ./ run-minifi . bat Lastly, on the receiving end, we'll start our TCP listener. 1 2 3 4 5 6 # From WSL \u276f nc -l 127 .0.0.1 8514 2021 -07-28T21:35:05 BoldDesktop Hello, World! <Event xmlns = 'http://schemas.microsoft.com/win/2004/08/events/event' ><System><Provider Name = 'Microsoft-Windows-Security-SPP' Guid = '{E23B33B0-C8C9-472C-A5F9-F2BDFEA0F156}' EventSourceName = 'Software Protection Platform Service' /><EventID Qualifiers = '49152' >16394</EventID><Version>0</Version><Level>4</Level><Task>0</Task><Opcode>0</Opcode><Keywords>0x80000000000000</Keywords><TimeCreated SystemTime = '2021-07-29T04:37:49.7185935Z' /><EventRecordID>3230</EventRecordID><Correlation/><Execution ProcessID = '0' ThreadID = '0' /><Channel>Application</Channel><Computer>BoldDesktop</Computer><Security/></System><EventData></EventData></Event><Event xmlns = 'http://schemas.microsoft.com/win/2004/08/events/event' ><System><Provider Name = 'SecurityCenter' /><EventID Qualifiers = '0' >15</EventID><Version>0</Version><Level>4</Level><Task>0</Task><Opcode>0</Opcode><Keywords>0x80000000000000</Keywords><TimeCreated SystemTime = '2021-07-29T04:38:03.9292335Z' /><EventRecordID>3231</EventRecordID><Correlation/><Execution ProcessID = '0' ThreadID = '0' /><Channel>Application</Channel><Computer>BoldDesktop</Computer><Security/></System><EventData><Data>Windows Defender</Data><Data>SECURITY_PRODUCT_STATE_ON</Data></EventData></Event><Event xmlns = 'http://schemas.microsoft.com/win/2004/08/events/event' ><System><Provider Name = 'Microsoft-Windows-Security-SPP' Guid = '{E23B33B0-C8C9-472C-A5F9-F2BDFEA0F156}' EventSourceName = 'Software Protection Platform Service' /><EventID Qualifiers = '16384' >16384</EventID><Version>0</Version><Level>4</Level><Task>0</Task><Opcode>0</Opcode><Keywords>0x80000000000000</Keywords><TimeCreated SystemTime = '2021-07-29T04:38:23.8359934Z' /><EventRecordID>3232</EventRecordID><Correlation/><Execution ProcessID = '0' ThreadID = '0' /><Channel>Application</Channel><Computer>BoldDesktop</Computer><Security/></System><EventData><Data>2121-07-05T04:38:23Z</Data><Data>RulesEngine</Data></EventData></Event>2021-07-28T21:35:07 BoldDesktop Hello, World! 2021 -07-28T21:35:09 BoldDesktop Hello, World! 2021 -07-28T21:35:11 BoldDesktop Hello, World! Attention The steps to convert the NiFi .xml template to the .yml config file have been omitted. Cloud Setup Leaving this as an exercise for the brave. While I've done this in the past, revisiting the subject has made me realize the tool is not suited for these quick evaluation scenarios. Conclusion This evaluation took longer than expected. Though all three tools are great products, each has their own unique advantages and disadvantages in comparison. Tool Documentation Ease of Use Cloud Readiness Architecture Fluentbit NXLog MiNiFi Fluentbit - Fluentbit is the youngest on the block with lots of momentum. It's built for rapid prototyping and minimal enough to meet a majority of use cases while still being easy to use. Relatively, documentation is less mature than the others but still isn't bad. Architecture is simple, but loses out in comparison to its predecessors. It makes up for this in user friendliness and cloud readiness (Kubernetes includes FluentD in its logging strategy documentation!) NXLog - Solidly in the middle ground. Old tool with a mature community. The NXLog language gives the user flexibility yet doesn't require in depth knowledge to use. MiNiFi - Documentation and Architecture are solid for the NiFi project. I will continue to advocate that this is hands down one of the best tools for dataflow management. However, the overhead of maintaining this platform is not for the startup or tinkerer. Cloudera does provide a solution that simplifies the management of agents and deployment of configurations, but it is behind a pay wall. If you're in an enterprise setting and you're keen on a robust dataflow strategy, it may be worth the time investment. Otherwise, stick to one of the other two. References Fluentbit Documentation NXLog Documentation MiNiFi Documentation Kubernetes Logging Architecture Minikube","title":"Agent Based Logging"},{"location":"blog/2021-07-25-agent-based-logging.html#agent-based-logging","text":"Posted by Jason Bolden on Jul 25, 2021 Whether you're working in site reliability, business intelligence, or security, one thing rings true. Logs are king. They are a window into your operations, providing insights into access, change, performance, who, what, when, where, and why. However, just as logs can be invaluable they can just as easily be burdensome and costly. Poor log hygiene plagues many organizations. While many business functions benefit from logs, they don't benefit from all logs and the excess translates directly to cost. In this post we'll explore some options for logging instrumentation that aid in filtering, routing, and maintaining the flow of logs at the host level.","title":"Agent Based Logging"},{"location":"blog/2021-07-25-agent-based-logging.html#objective","text":"In this post we'll explore 3 popular logging agents to better understand the pros and cons of each solution for different scenarios. The contenders are: Created in 2011, Fluentd is one of the most popular logging agents in the cloud computing space. The project was open-sourced in the same year, and Google supports their own flavor as their standard cloud logging agent. In 2014, Fluentbit was created as a lighter weight agent for IoT workloads. NXLog was created back in 2009 as an alternative to msyslog. Originally a closed source project, NXLog Community Edition was open-sourced in 2011 and has been free since. NXLog has a reputation in the cybersecurity space as a windows event log collector. SANS' SEC555 course references NXLog as a reliable free option for gaining visibility into windows and system logging. Apache NiFi began as a project created by the NSA. It was later introduced to the Apache Software Foundation, and subsequently commercialized by Hortonworks (now Cloudera ). The tool is a robust Data Flow Controller with the goal of making the automation and management of ETL processes simpler and more maintainable. MiNiFi is a sub-project that borrows the fundamental concepts defined by NiFi, but packages them in a smaller form factor for deployment to endpoints and IoT devices. We'll run through a basic deployment to a Windows desktop to demonstrate local setup and Kubernetes for cloud. In my personal experience, I've leveraged NiFi quite extensively. To be as objective as possibly in our evaluation, we'll measure each tool according to the following criteria: Documentation - This will be judged on completeness, number of examples, and searchability Ease of Use - How short is the time to get up and running, does the agent support monitoring, is it easy to maintain? Cloud Readiness - What cloud providers are supported? Does the documentation describe cloud installation? Architecture - How well is the agent designed, how resilient is it to operational disruption, can the base functionality be easily extended? For the sake of time, evaluating ease of use and documentation will be based on my ability to set up the agents after 30 minutes of reading the documentation. The following Figures depict our lab setup. Figure 1 - Conceptual Local Setup Figure 2 - Conceptual Cloud Setup All configurations and scripts for this post can be found in this GitHub repo . Tip When investigating multiple tools that solve the same problem, it's always a good idea to use time boxes. While 30 minutes is a bit extreme, it's not uncommon to dedicate a day to hacking out an MVP to assess the feasibility of using one solution over another.","title":"Objective"},{"location":"blog/2021-07-25-agent-based-logging.html#the-breakdown","text":"Logging agents typically follow a 3 part architecture. Source , defines how the agent interfaces with the log producing system. Channel , a transport layer. Data are stored, transformed, and/or filtered in this layer. Lastly, Sink defines the interface with the destination of the data. As we describe the architecture of the three Solutions, we'll tie their components back to these three constructs for consistency and ease of comparison. Figure 3 - Conceptual 3 part architecture for logging agents Note The astute would recognize this terminology from the Apache Flume project. This was the first logging agent I used in a production setting. Many projects have sprung up since, but their underlying architecture remains more or less the same. We'll also be using this short python script to produce our applications logs in each test. 1 2 3 4 5 6 7 8 from datetime import datetime import time , socket , sys while True : with open ( sys . argv [ 1 ], 'a' ) as f : f . write ( f ' { datetime . now () : %Y-%m-%dT%H:%M:%S } { socket . gethostname () } Hello, World! \\n ' ) f . close () time . sleep ( 2 ) This script is used in both the Local and Cloud setups. The following is the Dockerfile definition for our chatty-app. 1 2 3 4 5 6 7 FROM python:3.8-slim-buster WORKDIR /app COPY log-gen.py log-gen.py CMD [ \"python3\" , \"log-gen.py\" , \"/var/log/log-file.log\" ] Minikube and Docker should be installed on your machine if you intend to follow along. chatty-app must be built with the minikube environment variables set beforehand. 1 2 3 4 5 minikube start eval $( minikube docker-env ) cd logging-agent-eval/cloud docker build --tag chatty-app . docker images","title":"The Breakdown"},{"location":"blog/2021-07-25-agent-based-logging.html#fluentbit","text":"Fluentbit doesn't deviate much from our established formula. Configuration of the agent boasts a simplistic format and schema , utilizing Sections and indented key/value paired Entries Figure 4 - Fluentbit Data Pipeline Model","title":"Fluentbit"},{"location":"blog/2021-07-25-agent-based-logging.html#30-minute-sprint","text":"After 30 minutes of combing the documentation, I felt pretty comfortable to start hacking away at an MVP . The docs are organized using GitBook , making it fairly painless to navigate. If I had to be nit-picky, there's quite a few grammatical errors throughout. It's not unreadable, but it does throw one off a bit. Concerning the bells and whistles, security is supported through the use of TLS. All outputs that require network I/O support options for TLS configuration. Agents are resilient through the use of buffering , enabling persistance to disk. Agent health can be monitored via API calls when configured. Alternatively, one could use the Prometheus Exporter output plugin to route monitoring metrics directly to a prometheus server. configuration . Fluentbit also has a handy visualizer to aid in flow development. This can be very useful when troubleshooting complex dataflows. Attention Take care with buffering strategies on cloud workloads. Due to the ephemeral nature of container resources, one should ensure the Fluentbit storage paths point to persistent volumes in your K8S deployment.","title":"30 Minute Sprint"},{"location":"blog/2021-07-25-agent-based-logging.html#local-setup","text":"After downloading the ZIP archive for windows, create two new *.conf files to save the following settings: fluent-bit.conf defines our data flow. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [SERVICE] flush 5 daemon Off log_level info parsers_file parsers.conf [INPUT] name tail tag log_file path ../log-file.log parser custom [INPUT] name winlog tag win_log channels Security Interval_Sec 1 db winlog.sqlite [OUTPUT] name tcp match * host 127.0.0.1 port 8514 format json_lines parsers.conf holds our parsing definitions. In our case, we only define one parser to handle logs from the log-gen.py script. 1 2 3 4 5 6 [PARSER] Name custom Format regex Regex ^(?<time>[^ ]*) (?<host>[^ ]*) (?<message>.*)?$ Time_Key time Time_Format %Y-%m-%dT%H:%M:%S Attention Ensure that the tail INPUT path parameter matches the path used by the log-gen.py script. With our config files in hand, we can run the following commands to start our test. Run the logger script. 1 \u276f python log-gen . py log -file . log In another terminal, with administrative privileges. 1 2 \u276f cd td-agent-bit -* \u276f ./ bin / fluent-bit . exe -c ./ conf / fluent-bit . conf Lastly, on the receiving end, we'll start our TCP listener. 1 2 3 4 5 6 7 # From WSL \u276f nc -l 127 .0.0.1 8514 { \"date\" :1627405959.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405961.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405963.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405965.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } { \"date\" :1627405967.0, \"host\" : \"BoldDesktop\" , \"message\" : \"Hello, World!\" } Note I've left out the windows event logs from the samples above to minimize sharing of sensitive information.","title":"Local Setup"},{"location":"blog/2021-07-25-agent-based-logging.html#cloud-setup","text":"More time was spent configuring the chatty-app container and troubleshooting minikube than actually configuring the fluent-bit container. The config files for this setup are listed below: fluent-bit.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [SERVICE] flush 5 daemon Off log_level info parsers_file parsers.conf [INPUT] name tail tag log_file path /var/log/log-file.log parser custom [OUTPUT] name stdout match * parser.conf is the same as before. cloud-logging-fluentbit.yml is our pod configuration for K8S 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : v1 kind : Pod metadata : name : test-pod spec : containers : - name : chatty-app image : chatty-app imagePullPolicy : Never volumeMounts : - name : varlog mountPath : /var/log - name : fluent-bit image : fluent/fluent-bit volumeMounts : - name : varlog mountPath : /var/log - name : config mountPath : /fluent-bit/etc volumes : - name : varlog emptyDir : {} - name : config configMap : name : fluentbit-configmap The fluentbit container config mounts our user defined configmap to the expected directory. To deploy run the following. 1 2 3 \u276f kubectl create configmap fluentbit-configmap --from-file = fluent-bit.conf --from-file = parsers.conf \u276f kubectl apply -f ./cloud-logging-fluentbit.yml \u276f minikube dashboard & Navigate to the provisioned pod and view the logs of the fluent-bit container. Figure 5 - Log output from the fluent-bit container in the K8S UI Note Fluentd and Kubernetes both document using Fluentbit agents in a daemonset configuration rather than a sidecar. For consistency in our testing we will not follow that recommendation; however, the sidecar configuration is still a recommended logging pattern documented by Kubernetes.","title":"Cloud Setup"},{"location":"blog/2021-07-25-agent-based-logging.html#nxlog","text":"NXLog may appear simple conceptually ; however, each component is very extensible. Figure 6 - NXLog Conceptual Architecture","title":"NXLog"},{"location":"blog/2021-07-25-agent-based-logging.html#30-minute-sprint_1","text":"NXLog is definitely a very mature project. The documentation is dense and thorough. When I stumbled on their expression language page, I knew I was headed into power user territory. NXLog Routes process event records , collections of fields . The configuration file is made up of 2 main constructs, Directives and Modules . There are 4 types of Modules . Inputs consume event records from source systems and optionally parse incoming records. Processors provide functionality for transforming, buffering, and/or filtering event records. Outputs emit event records to a destination system. Lastly, Extensions provide extended functionality to the NXLog language. Directives are parameters that define the various components of the NXLog configuration. Several examples of buffering strategies are documented. Monitoring functionality is lackluster . Documentation only mentions OS specific methods for ensuring NXLog runs as a service; however, collection of health metrics is not mentioned. There are several useful features that are only available in the enterprise version of the agent. Their feature comparison section goes into more depth. In all, solid knowledge base backing the project. Almost overwhelming, but not so much so that we can't get an MVP running quickly. Security is supported via Input and Output modules that handle SSL/TLS configurations.","title":"30 Minute Sprint"},{"location":"blog/2021-07-25-agent-based-logging.html#local-setup_1","text":"After downloading the installer and completing installation , create one new *.conf files to save the following settings: nxlog.conf defines our data flow. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 <Extension _json> Module xm_json </Extension> <Input winlog> Module im_wseventing Exec to_json(); <QueryXML> <QueryList> <Query Id=\"0\" Path=\"Application\"> <Select Path=\"System\">*</Select> </Query> </QueryList> </QueryXML> </Input> define EVENT_REGEX /(?x)^(\\d+-\\d+-\\d+T\\d+:\\d+:\\d+)\\s+(\\S+)\\s+(.*)/ <Input file> Module im_file File \"C:\\\\logs\\\\log-file.log\" <Exec> if $raw_event =~ %EVENT_REGEX% { $EventTime = strptime($1,'%Y-%m-%dT%T'); $Host = $2; $Message = $3; to_json(); } else drop(); </Exec> </Input> <Output tcp> Module om_tcp Host 127.0.0.1 Port 8514 </Output> <Route winlog_to_tcp> Path winlog => tcp </Route> <Route file_to_tcp> Path file => tcp </Route> Attention Ensure that the File directive matches the path used by the log-gen.py script. Testing procedures are as follows. Run the logger script. 1 \u276f python log-gen . py log -file . log In another terminal, with administrative privileges. 1 2 \u276f cd $NXLOG_PATH \u276f ./ nxlog . exe -c ./ nxlog . conf Lastly, on the receiving end, we'll start our TCP listener. 1 2 3 4 5 6 # From WSL \u276f nc -l 127 .0.0.1 8514 { \"EventReceivedTime\" : \"2021-07-27 22:34:56\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:34:56\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } { \"EventReceivedTime\" : \"2021-07-27 22:34:58\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:34:58\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } { \"EventReceivedTime\" : \"2021-07-27 22:35:00\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:35:00\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } { \"EventReceivedTime\" : \"2021-07-27 22:35:02\" , \"SourceModuleName\" : \"file\" , \"SourceModuleType\" : \"im_file\" , \"EventTime\" : \"2021-07-27 22:35:02\" , \"Host\" : \"BoldDesktop\" , \"Message\" : \"Hello, World!\" } Attention NXLog for Windows is an .msi installation. For my setup, I added the path to the nxlog.exe to my system path for ease of use. $NXLOG_PATH is not defined by default. Note Again, I've left out the windows event logs from the samples above to minimize sharing of sensitive information.","title":"Local Setup"},{"location":"blog/2021-07-25-agent-based-logging.html#cloud-setup_1","text":"The K8S config for NXLog is a bit different than for Fluentbit. Instead of writing the logs to stdout , we've configured NXLog to write them to the agent's internal log file. NXLog does not support writing to stdout . NXLog documentation provides instructions to build the docker image for the agent locally; however, it can be pull remotely from DockerHub. The config files for this setup are listed below: nxlog.conf - Logs are output to the om_null module, and we use the log_info() function to capture them instead 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 User nxlog Group nxlog LogFile /var/log/nxlog.log LogLevel INFO <Extension _json> Module xm_json </Extension> define EVENT_REGEX /(?x)^(\\d+-\\d+-\\d+T\\d+:\\d+:\\d+)\\s+(\\S+)\\s+(.*)/ <Input file> Module im_file File \"/var/log/log-file.log\" <Exec> if $raw_event =~ %EVENT_REGEX% { $EventTime = strptime($1,'%Y-%m-%dT%T'); $Host = $2; $Message = $3; log_info(to_json()); } else drop(); </Exec> </Input> <Output null> Module om_null </Output> <Route file_to_tcp> Path file => null </Route> cloud-logging-nxlog.yml is our pod configuration for K8S 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion : v1 kind : Pod metadata : name : test-pod spec : containers : - name : chatty-app image : chatty-app imagePullPolicy : Never volumeMounts : - name : varlog mountPath : /var/log - name : nxlog-ce image : nxlog/nxlog-ce args : [ \"-c\" , \"/etc/nxlog/nxlog.conf\" ] volumeMounts : - name : varlog mountPath : /var/log - name : config mountPath : /etc/nxlog volumes : - name : varlog emptyDir : {} - name : config configMap : name : nxlog-configmap Like before, the NXLog container config mounts our user defined configmap to the expected directory. In this case, the path to the config is passed as a container argument. To deploy run the following. 1 2 3 \u276f kubectl create configmap nxlog-configmap --from-file = nxlog.conf \u276f kubectl apply -f ./cloud-logging-nxlog.yml \u276f minikube dashboard & Navigate to the provisioned pod and start the terminal for the nxlog-ce container. Execute tail -f /var/log/nxlog.log to see the agent logs. Figure 7 - Log output from the nxlog-ce container in the K8S UI Note NXLog documents both Daemonset and Sidecar configurations for logging.","title":"Cloud Setup"},{"location":"blog/2021-07-25-agent-based-logging.html#minifi","text":"Coined the Swiss Army Knife of Dataflows, there's almost nothing you can't do with Apache NiFi. Eat your heart out on the documentation . Figure 8 - MiNiFi conceptual architecture","title":"MiNiFi"},{"location":"blog/2021-07-25-agent-based-logging.html#30-minute-sprint_2","text":"NiFi/MiNiFi uses Processors to create, transform, and transmit Flowfiles . That's pretty much it. Full disclosure, I have many years of experience working with NiFi. I've deployed and maintained NiFi clusters in multiple data centers, developed configurations for MiNiFi in the cloud, and deployed agents to thousands of self service machines. I'm no stranger to the tool, so the 30 minutes time cap doesn't really apply here. That said, my experience with this tool did not help me as much as it should have for our two test cases. For the sake of time and, frankly, to limit the scope of this post, I had to cut the testing of the MiNiFi agent short as I was going way too far in the weeds trying to get the configuration working. While NiFi and MiNiFi sport all the features you'd want in an enterprise setting ( security , resiliency , monitoring , flexibility, extensibility, etc.), the tool is massive and requires a lot of overhead to utilize effectively. Not to add insult to injury, but MiNiFi has two version, C++ and Java. The Java agent, for all intensive purposes, is a headless version of NiFi with fewer core packages included. It's still a hefty agent, but you have 100% feature parity with the server version. The C++ agent is much lighter, however not all functionality is supported.","title":"30 Minute Sprint"},{"location":"blog/2021-07-25-agent-based-logging.html#local-setup_2","text":"MiNiFi agents are configured using as single yaml ; however, it is not recommended that one writes a configuration from a text editor (you'll soon see why). The Quickstart walks through how to create a dataflow from the NiFi UI, save it as a template, export, and convert to the .yml config accepted by MiNiFi. Figure 9 - NiFi UI depicting local dataflow setup After conversion, the flow above becomes... conf.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 MiNiFi Config Version : 3 Flow Controller : name : local-setup comment : '' Core Properties : flow controller graceful shutdown period : 10 sec flow service write delay interval : 500 ms administrative yield duration : 30 sec bored yield duration : 10 millis max concurrent threads : 1 variable registry properties : '' FlowFile Repository : partitions : 256 checkpoint interval : 2 mins always sync : false Swap : threshold : 20000 in period : 5 sec in threads : 1 out period : 5 sec out threads : 4 Content Repository : implementation : org.apache.nifi.controller.repository.FileSystemRepository content claim max appendable size : 10 MB content claim max flow files : 100 content repository archive enabled : false content repository archive max retention period : 12 hours content repository archive max usage percentage : 50% always sync : false Provenance Repository : provenance rollover time : 1 min implementation : org.apache.nifi.provenance.WriteAheadProvenanceRepository provenance index shard size : 500 MB provenance max storage size : 1 GB provenance max storage time : 24 hours provenance buffer size : 10000 Component Status Repository : buffer size : 1440 snapshot frequency : 1 min Security Properties : keystore : '' keystore type : '' keystore password : '' key password : '' truststore : '' truststore type : '' truststore password : '' ssl protocol : '' Sensitive Props : key : gAHRSHkEd0lN7Vy1SW20sgZhsNjyGSWx algorithm : PBEWITHMD5AND256BITAES-CBC-OPENSSL provider : BC Processors : - id : 4933996e-f262-3afa-0000-000000000000 name : ConsumeWindowsEventLog class : org.apache.nifi.processors.windows.event.log.ConsumeWindowsEventLog max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : [] Properties : channel : Application inactiveDurationToReconnect : 10 mins maxBuffer : '1048576' maxQueue : '1024' query : |- <QueryList> <Query Id=\"0\" Path=\"Application\"> <Select Path=\"Application\">*</Select> </Query> </QueryList> - id : ffaea6de-2e7c-3964-0000-000000000000 name : PutElasticsearch class : org.apache.nifi.processors.elasticsearch.PutElasticsearch max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : [] Properties : Batch Size : '100' Character Set : UTF-8 Cluster Name : elasticsearch ElasticSearch Hosts : ElasticSearch Ping Timeout : 5s Identifier Attribute : Index : Index Operation : index Password : SSL Context Service : Sampler Interval : 5s Shield Plugin Filename : Type : Username : - id : a6cc363f-e088-3e94-0000-000000000000 name : PutTCP class : org.apache.nifi.processors.standard.PutTCP max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : - failure - success Properties : Character Set : UTF-8 Connection Per FlowFile : 'false' Hostname : localhost Idle Connection Expiration : 5 seconds Max Size of Socket Send Buffer : 1 MB Outgoing Message Delimiter : Port : '8514' SSL Context Service : Timeout : 10 seconds - id : 3ece5573-f82e-3f5f-0000-000000000000 name : TailFile class : org.apache.nifi.processors.standard.TailFile max concurrent tasks : 1 scheduling strategy : TIMER_DRIVEN scheduling period : 0 sec penalization period : 30 sec yield period : 1 sec run duration nanos : 0 auto-terminated relationships list : [] Properties : File Location : Local File to Tail : C:\\\\logs\\\\log-file.log Initial Start Position : Beginning of File Post-Rollover Tail Period : 0 sec Rolling Filename Pattern : reread-on-nul : 'false' tail-base-directory : tail-mode : Single file tailfile-lookup-frequency : 10 minutes tailfile-maximum-age : 24 hours tailfile-recursive-lookup : 'false' Controller Services : [] Process Groups : [] Input Ports : [] Output Ports : [] Funnels : [] Connections : - id : e7bee299-ce60-3591-0000-000000000000 name : ConsumeWindowsEventLog/success/PutTCP source id : 4933996e-f262-3afa-0000-000000000000 source relationship names : - success destination id : a6cc363f-e088-3e94-0000-000000000000 max work queue size : 10000 max work queue data size : 1 GB flowfile expiration : 0 sec queue prioritizer class : '' - id : b1d11f20-c37d-365e-0000-000000000000 name : TailFile/success/PutTCP source id : 3ece5573-f82e-3f5f-0000-000000000000 source relationship names : - success destination id : a6cc363f-e088-3e94-0000-000000000000 max work queue size : 10000 max work queue data size : 1 GB flowfile expiration : 0 sec queue prioritizer class : '' Remote Process Groups : [] NiFi Properties Overrides : {} Testing procedures are as follows. We're not going into great detail on this one. Run the logger script. 1 \u276f python log-gen . py log -file . log In another terminal, ensure that the config.yml file is in the conf directory for MiNiFi. Start the agent. 1 \u276f ./ run-minifi . bat Lastly, on the receiving end, we'll start our TCP listener. 1 2 3 4 5 6 # From WSL \u276f nc -l 127 .0.0.1 8514 2021 -07-28T21:35:05 BoldDesktop Hello, World! <Event xmlns = 'http://schemas.microsoft.com/win/2004/08/events/event' ><System><Provider Name = 'Microsoft-Windows-Security-SPP' Guid = '{E23B33B0-C8C9-472C-A5F9-F2BDFEA0F156}' EventSourceName = 'Software Protection Platform Service' /><EventID Qualifiers = '49152' >16394</EventID><Version>0</Version><Level>4</Level><Task>0</Task><Opcode>0</Opcode><Keywords>0x80000000000000</Keywords><TimeCreated SystemTime = '2021-07-29T04:37:49.7185935Z' /><EventRecordID>3230</EventRecordID><Correlation/><Execution ProcessID = '0' ThreadID = '0' /><Channel>Application</Channel><Computer>BoldDesktop</Computer><Security/></System><EventData></EventData></Event><Event xmlns = 'http://schemas.microsoft.com/win/2004/08/events/event' ><System><Provider Name = 'SecurityCenter' /><EventID Qualifiers = '0' >15</EventID><Version>0</Version><Level>4</Level><Task>0</Task><Opcode>0</Opcode><Keywords>0x80000000000000</Keywords><TimeCreated SystemTime = '2021-07-29T04:38:03.9292335Z' /><EventRecordID>3231</EventRecordID><Correlation/><Execution ProcessID = '0' ThreadID = '0' /><Channel>Application</Channel><Computer>BoldDesktop</Computer><Security/></System><EventData><Data>Windows Defender</Data><Data>SECURITY_PRODUCT_STATE_ON</Data></EventData></Event><Event xmlns = 'http://schemas.microsoft.com/win/2004/08/events/event' ><System><Provider Name = 'Microsoft-Windows-Security-SPP' Guid = '{E23B33B0-C8C9-472C-A5F9-F2BDFEA0F156}' EventSourceName = 'Software Protection Platform Service' /><EventID Qualifiers = '16384' >16384</EventID><Version>0</Version><Level>4</Level><Task>0</Task><Opcode>0</Opcode><Keywords>0x80000000000000</Keywords><TimeCreated SystemTime = '2021-07-29T04:38:23.8359934Z' /><EventRecordID>3232</EventRecordID><Correlation/><Execution ProcessID = '0' ThreadID = '0' /><Channel>Application</Channel><Computer>BoldDesktop</Computer><Security/></System><EventData><Data>2121-07-05T04:38:23Z</Data><Data>RulesEngine</Data></EventData></Event>2021-07-28T21:35:07 BoldDesktop Hello, World! 2021 -07-28T21:35:09 BoldDesktop Hello, World! 2021 -07-28T21:35:11 BoldDesktop Hello, World! Attention The steps to convert the NiFi .xml template to the .yml config file have been omitted.","title":"Local Setup"},{"location":"blog/2021-07-25-agent-based-logging.html#cloud-setup_2","text":"Leaving this as an exercise for the brave. While I've done this in the past, revisiting the subject has made me realize the tool is not suited for these quick evaluation scenarios.","title":"Cloud Setup"},{"location":"blog/2021-07-25-agent-based-logging.html#conclusion","text":"This evaluation took longer than expected. Though all three tools are great products, each has their own unique advantages and disadvantages in comparison. Tool Documentation Ease of Use Cloud Readiness Architecture Fluentbit NXLog MiNiFi Fluentbit - Fluentbit is the youngest on the block with lots of momentum. It's built for rapid prototyping and minimal enough to meet a majority of use cases while still being easy to use. Relatively, documentation is less mature than the others but still isn't bad. Architecture is simple, but loses out in comparison to its predecessors. It makes up for this in user friendliness and cloud readiness (Kubernetes includes FluentD in its logging strategy documentation!) NXLog - Solidly in the middle ground. Old tool with a mature community. The NXLog language gives the user flexibility yet doesn't require in depth knowledge to use. MiNiFi - Documentation and Architecture are solid for the NiFi project. I will continue to advocate that this is hands down one of the best tools for dataflow management. However, the overhead of maintaining this platform is not for the startup or tinkerer. Cloudera does provide a solution that simplifies the management of agents and deployment of configurations, but it is behind a pay wall. If you're in an enterprise setting and you're keen on a robust dataflow strategy, it may be worth the time investment. Otherwise, stick to one of the other two.","title":"Conclusion"},{"location":"blog/2021-07-25-agent-based-logging.html#references","text":"Fluentbit Documentation NXLog Documentation MiNiFi Documentation Kubernetes Logging Architecture Minikube","title":"References"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html","text":"AUTHOR var article = document.getElementsByTagName(\"article\")[0]; article.insertBefore(document.getElementById(\"banner\"), article.childNodes[0]); Hello, GitHub Discussions! Goodbye, Disqus... Posted by Jason Bolden on Nov 02, 2021 In short, I don't like the free version of Disqus. Ads serve a purpose in this world, and they often allow us to enjoy a lot of valuable content at the cost of a portion of our attention. However, I don't want them on my blog. Instead, let's leverage some of the native functionality of GitHub and create a Discussion thread when a new post is published to the blog; all automated, of course. Objective Referencing the previous post on Documentation Automation , we intend to make a minor change to the existing flow. The reason why one would want to do this is to further consolidate the developer experience by adding a forum for collaborative communication linked to published content within the project. Figure 1 - Conceptual Flow The Breakdown We'll start by explaining all the pieces necessary to add the new automation steps and tie them all together in the end. GitHub Discussions GraphQL API GitHub recently released their GraphQL API for Discussions . For our workflow, we want to create a new Discussion when a new Blog Post issue is assigned. To figure out how to programmatically do this, let's reference the docs and leverage the GitHub GraphQL Explorer . Figure 2 - GitHub GraphQL Explorer The details behind GraphQL fall outside the scope of this post; however GitHub provides a nice overview on their docs site. The calls we need to perform are as follows: query for the repositoryId and categoryId we want to assign the new discussion mutation to create the new discussion Figure 3 - GraphQL DiscussionCategories Query 1 2 3 4 5 6 7 8 9 10 11 query DiscussionCategory { repository(name: \"profile\", owner: \"jbold569\") { discussionCategories(first: 10) { nodes { name id } } id } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"data\" : { \"repository\" : { \"discussionCategories\" : { \"nodes\" : [ { \"name\" : \"Announcements\" , \"id\" : \"DIC_kwDOFjAb884B_pJC\" }, { \"name\" : \"General\" , \"id\" : \"DIC_kwDOFjAb884B_pJD\" }, { \"name\" : \"Q&A\" , \"id\" : \"DIC_kwDOFjAb884B_pJE\" }, { \"name\" : \"Ideas\" , \"id\" : \"DIC_kwDOFjAb884B_pJF\" }, { \"name\" : \"Show and tell\" , \"id\" : \"DIC_kwDOFjAb884B_pJG\" }, { \"name\" : \"Polls\" , \"id\" : \"DIC_kwDOFjAb884B_pJH\" }, { \"name\" : \"Blog\" , \"id\" : \"DIC_kwDOFjAb884B_pOx\" } ] }, \"id\" : \"MDEwOlJlcG9zaXRvcnkzNzIyNTE2MzU=\" } } } The GraphQL explorer returns a JSON payload with information required for the next step. Without this step, The Mutation call wouldn't know which repository to create the new Discussion in nor what category to assign it. 1 2 3 4 5 6 7 8 mutation CreateDiscussion { __typename createDiscussion(input: {repositoryId: \"MDEwOlJlcG9zaXRvcnkzNzIyNTE2MzU=\", title: \"Explorer Test\", body: \"This is a discussion made via the GraphQL Explorer!\", categoryId: \"DIC_kwDOFjAb884B_pOx\", clientMutationId: \"\"}) { discussion { number } } } 1 2 3 4 5 6 7 8 9 10 { \"data\" : { \"__typename\" : \"Mutation\" , \"createDiscussion\" : { \"discussion\" : { \"number\" : 15 } } } } Figure 4 - Discussion created with GraphQL Attention On line 5 of the mutation snippet, we specified to return the value of the number field for the new discussion. Having the discussion number will be useful in a later enhancement to this flow. I'll expand more on this at the conclusion of this post. GitHub GraphQL Action Now that we've establish what API calls we need to make the discussion and tested them manually, let's build out the automation to add this to our pipeline. The steps we want to inject into our workflow are as follows: Figure 6 - New action steps Octokit maintains a repo, graphql-action , that allows you to make calls to the GitHub GraphQL API via a GitHub Action. At this time, it's limited to only GraphQL queries, so we'll also explore how to make calls using cURL for the mutation portion. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - name : Query GraphQL for Ids uses : octokit/graphql-action@v2.x id : get_ids with : query : | query ids($owner:String!,$repo:String!) { repository(owner:$owner,name:$repo) { discussionCategories(first: 10) { nodes { name id } } id } } owner : ${{ github.event.repository.owner.login }} repo : ${{ github.event.repository.name }} env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} The octokit/graphql-action allows us to basically copy/paste the query we developed in the explorer with the exception of defining the variables and their types being passed to the query; similar to a function definition. Lines 17 and 18 pull the repository name and owner login from the action event object rather than hardcoded values seen previously. Attention The octokit/graphql-action documentation references the use of github.event.repository.owner.name . That value is not present in the webhook event payload object . It should be github.event.repository.owner.login The question now is, how do we pull the Id's from the json returned by the initial query? Introducing, jq . jq is a handy command-line utility that makes processing JSON a easier. We're going to use it to extract the Id's and assign them to environment variables for our next workflow step. Figure 5 - jqplay query builder output 1 2 3 4 5 6 7 - name : Extract Ids id : extract_ids env : JSON_DATA : ${{ steps.get_ids.outputs.data}} run : > echo \"repositoryId=$(echo ${JSON_DATA} | jq '.repository.id')\" >> $GITHUB_ENV echo \"categoryId=$(echo ${JSON_DATA} | jq '.repository.discussionCategories.nodes[] | select(.name == \"Blog\") | .id')\" >> $GITHUB_ENV At the time of writing, the octokit/graphql-action action did not support mutations \ud83d\ude22. Luckily there's a fallback. The GitHub GraphQL API can be queried using http requests . Knowing this, we can instead form a JSON payload containing our mutation statement and send it via cURL. It's messy (lots of debugging escape-characters), but it gets the job done. 1 2 3 4 5 6 7 8 9 10 11 12 13 - name : Create Discussion cURL id : create_discussion run : | curl -H \"Authorization: bearer ${{env.GITHUB_TOKEN }}\" -X POST -d \" \\ { \\ \\\"query\\\": \\\"mutation { createDiscussion(input: {repositoryId: \\\\\\\"${{ env.repositoryId }}\\\\\\\", title: \\\\\\\"${{ env.title }}\\\\\\\", body: \\\\\\\"${{ env.body }}\\\\\\\", categoryId: \\\\\\\"${{ env.categoryId }}\\\\\\\"}) { discussion { number } } }\\\" \\ } \\ \" https://api.github.com/graphql env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} title : ${{ github.event.issue.title }} body : \"This discussion was made by GitHub Actions\" Figure 6 - Discussion create by the 3 new action steps Conclusion We'll stop here for the scope of this post. There are some other things that could be added to further enhance the automation: Inject the url to the new discussion in the profile_builder module so it can be added to the new blog post template automatically. Discussions urls match the pattern: /<owner>/<repository>/discussions/<discussion_number> (recall that we purposefully captured that number after the mutation call) Programmatically add the link to the new Blog Post to the Discussion Body. Assign different discussion categories or use a different body template based on issue labels Let's take a look at the bigger picture for a second. Discussions were created to facilitate collaborative communication about a project. Assuming you're a maintainer for a project on GitHub and you have a project site for publishing posts, tying these posts to a discussion just makes sense if you value community around your product. The bonus here being, Discussions are built into the GitHub ecosystem and are tied directly to your project. Fewer dependencies, less context switching, it's awesome! References GitHub GraphQL API for Discussions GitHub GraphQL Intro jq GitHub Webhook Events and Payloads GitHub GraphQL cURL Comment Continue the discussion here !","title":"Hello, GitHub Discussions! Goodbye, Disqus..."},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#hello-github-discussions-goodbye-disqus","text":"Posted by Jason Bolden on Nov 02, 2021 In short, I don't like the free version of Disqus. Ads serve a purpose in this world, and they often allow us to enjoy a lot of valuable content at the cost of a portion of our attention. However, I don't want them on my blog. Instead, let's leverage some of the native functionality of GitHub and create a Discussion thread when a new post is published to the blog; all automated, of course.","title":"Hello, GitHub Discussions! Goodbye, Disqus..."},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#objective","text":"Referencing the previous post on Documentation Automation , we intend to make a minor change to the existing flow. The reason why one would want to do this is to further consolidate the developer experience by adding a forum for collaborative communication linked to published content within the project. Figure 1 - Conceptual Flow","title":"Objective"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#the-breakdown","text":"We'll start by explaining all the pieces necessary to add the new automation steps and tie them all together in the end.","title":"The Breakdown"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#github-discussions-graphql-api","text":"GitHub recently released their GraphQL API for Discussions . For our workflow, we want to create a new Discussion when a new Blog Post issue is assigned. To figure out how to programmatically do this, let's reference the docs and leverage the GitHub GraphQL Explorer . Figure 2 - GitHub GraphQL Explorer The details behind GraphQL fall outside the scope of this post; however GitHub provides a nice overview on their docs site. The calls we need to perform are as follows: query for the repositoryId and categoryId we want to assign the new discussion mutation to create the new discussion Figure 3 - GraphQL DiscussionCategories Query 1 2 3 4 5 6 7 8 9 10 11 query DiscussionCategory { repository(name: \"profile\", owner: \"jbold569\") { discussionCategories(first: 10) { nodes { name id } } id } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"data\" : { \"repository\" : { \"discussionCategories\" : { \"nodes\" : [ { \"name\" : \"Announcements\" , \"id\" : \"DIC_kwDOFjAb884B_pJC\" }, { \"name\" : \"General\" , \"id\" : \"DIC_kwDOFjAb884B_pJD\" }, { \"name\" : \"Q&A\" , \"id\" : \"DIC_kwDOFjAb884B_pJE\" }, { \"name\" : \"Ideas\" , \"id\" : \"DIC_kwDOFjAb884B_pJF\" }, { \"name\" : \"Show and tell\" , \"id\" : \"DIC_kwDOFjAb884B_pJG\" }, { \"name\" : \"Polls\" , \"id\" : \"DIC_kwDOFjAb884B_pJH\" }, { \"name\" : \"Blog\" , \"id\" : \"DIC_kwDOFjAb884B_pOx\" } ] }, \"id\" : \"MDEwOlJlcG9zaXRvcnkzNzIyNTE2MzU=\" } } } The GraphQL explorer returns a JSON payload with information required for the next step. Without this step, The Mutation call wouldn't know which repository to create the new Discussion in nor what category to assign it. 1 2 3 4 5 6 7 8 mutation CreateDiscussion { __typename createDiscussion(input: {repositoryId: \"MDEwOlJlcG9zaXRvcnkzNzIyNTE2MzU=\", title: \"Explorer Test\", body: \"This is a discussion made via the GraphQL Explorer!\", categoryId: \"DIC_kwDOFjAb884B_pOx\", clientMutationId: \"\"}) { discussion { number } } } 1 2 3 4 5 6 7 8 9 10 { \"data\" : { \"__typename\" : \"Mutation\" , \"createDiscussion\" : { \"discussion\" : { \"number\" : 15 } } } } Figure 4 - Discussion created with GraphQL Attention On line 5 of the mutation snippet, we specified to return the value of the number field for the new discussion. Having the discussion number will be useful in a later enhancement to this flow. I'll expand more on this at the conclusion of this post.","title":"GitHub Discussions GraphQL API"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#github-graphql-action","text":"Now that we've establish what API calls we need to make the discussion and tested them manually, let's build out the automation to add this to our pipeline. The steps we want to inject into our workflow are as follows: Figure 6 - New action steps Octokit maintains a repo, graphql-action , that allows you to make calls to the GitHub GraphQL API via a GitHub Action. At this time, it's limited to only GraphQL queries, so we'll also explore how to make calls using cURL for the mutation portion. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - name : Query GraphQL for Ids uses : octokit/graphql-action@v2.x id : get_ids with : query : | query ids($owner:String!,$repo:String!) { repository(owner:$owner,name:$repo) { discussionCategories(first: 10) { nodes { name id } } id } } owner : ${{ github.event.repository.owner.login }} repo : ${{ github.event.repository.name }} env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} The octokit/graphql-action allows us to basically copy/paste the query we developed in the explorer with the exception of defining the variables and their types being passed to the query; similar to a function definition. Lines 17 and 18 pull the repository name and owner login from the action event object rather than hardcoded values seen previously. Attention The octokit/graphql-action documentation references the use of github.event.repository.owner.name . That value is not present in the webhook event payload object . It should be github.event.repository.owner.login The question now is, how do we pull the Id's from the json returned by the initial query? Introducing, jq . jq is a handy command-line utility that makes processing JSON a easier. We're going to use it to extract the Id's and assign them to environment variables for our next workflow step. Figure 5 - jqplay query builder output 1 2 3 4 5 6 7 - name : Extract Ids id : extract_ids env : JSON_DATA : ${{ steps.get_ids.outputs.data}} run : > echo \"repositoryId=$(echo ${JSON_DATA} | jq '.repository.id')\" >> $GITHUB_ENV echo \"categoryId=$(echo ${JSON_DATA} | jq '.repository.discussionCategories.nodes[] | select(.name == \"Blog\") | .id')\" >> $GITHUB_ENV At the time of writing, the octokit/graphql-action action did not support mutations \ud83d\ude22. Luckily there's a fallback. The GitHub GraphQL API can be queried using http requests . Knowing this, we can instead form a JSON payload containing our mutation statement and send it via cURL. It's messy (lots of debugging escape-characters), but it gets the job done. 1 2 3 4 5 6 7 8 9 10 11 12 13 - name : Create Discussion cURL id : create_discussion run : | curl -H \"Authorization: bearer ${{env.GITHUB_TOKEN }}\" -X POST -d \" \\ { \\ \\\"query\\\": \\\"mutation { createDiscussion(input: {repositoryId: \\\\\\\"${{ env.repositoryId }}\\\\\\\", title: \\\\\\\"${{ env.title }}\\\\\\\", body: \\\\\\\"${{ env.body }}\\\\\\\", categoryId: \\\\\\\"${{ env.categoryId }}\\\\\\\"}) { discussion { number } } }\\\" \\ } \\ \" https://api.github.com/graphql env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} title : ${{ github.event.issue.title }} body : \"This discussion was made by GitHub Actions\" Figure 6 - Discussion create by the 3 new action steps","title":"GitHub GraphQL Action"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#conclusion","text":"We'll stop here for the scope of this post. There are some other things that could be added to further enhance the automation: Inject the url to the new discussion in the profile_builder module so it can be added to the new blog post template automatically. Discussions urls match the pattern: /<owner>/<repository>/discussions/<discussion_number> (recall that we purposefully captured that number after the mutation call) Programmatically add the link to the new Blog Post to the Discussion Body. Assign different discussion categories or use a different body template based on issue labels Let's take a look at the bigger picture for a second. Discussions were created to facilitate collaborative communication about a project. Assuming you're a maintainer for a project on GitHub and you have a project site for publishing posts, tying these posts to a discussion just makes sense if you value community around your product. The bonus here being, Discussions are built into the GitHub ecosystem and are tied directly to your project. Fewer dependencies, less context switching, it's awesome!","title":"Conclusion"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#references","text":"GitHub GraphQL API for Discussions GitHub GraphQL Intro jq GitHub Webhook Events and Payloads GitHub GraphQL cURL","title":"References"},{"location":"blog/2021-11-02-hello-github-discussions-goodbye-disqus.html#comment","text":"Continue the discussion here !","title":"Comment"},{"location":"blog/2021-11-17-introducing-dochive.html","text":"var article = document.getElementsByTagName(\"article\")[0]; article.insertBefore(document.getElementById(\"banner\"), article.childNodes[0]); Introducing, DocHive! Posted by Jason Bolden on Nov 17, 2021 After reusing the code powering this blog across multiple projects, a definite pattern has come to light. At its core, the profile_builder module takes a list of key/value pairs as input, populates a documents template, and updates a mkdocs configuration. This can be leveraged across a multitude of projects to drive many use cases. It only makes sense to rip out the code and turn it into a proper piece of software. This is the start of the document archiver, DocHive . Conceptual DocHive is intended to be a simple utility. Given three inputs, DocHive will output a new document for your archive and update the nav section of a mkdocs config file. For now, DocHive will be built to compliment mkdocs and leverages Jinja for templates. DocHive is not a cookiecutter replacement. Figure 1 - DocHive Conceptual General Usage Continuing with the theme of simplicity, DocHive will have 2 commands initially. add The add command creates a new document and adds it to the archive. The following options describe ways to manipulate the generated document and how mkdocs should navigate to it. Figure 2 - Proposed options for add Description of proposed command options: --template - jinja template to render the new doc --config-file - yaml file containing key/value pairs to populate the template --config - inline key/value pair to input on the commandline --nav - navigation path in the mkdocs config to add the new doc (supports date format codes) --timestamp - timestamp used to populate date format codes (defaults to time of execution) --output-dir - directory to output generated doc --mkdocs-file - mkdocs yaml file to update with new doc addition digest The digest command reads metadata from the last N documents in the archive and populates a template file using them. The immediate use case this addresses is, for example, a \"Recent Posts\" section of a blog page. The digest command can be used to populate cards for the 3 most recent posts to the blog. The metadata is simply yaml formatted data at the top of the document in a --- blocked section. For example: 1 2 3 4 5 6 --- title : Introducing, DocHive! description : After reusing the code powering this blog across multiple projects, a definite pattern has come to light. At its core, the [`profile_builder`](https://jbold569.github.io/profile/blog/2021-07-17-automating-docs-as-code.html) module takes a list of key/value pairs as input, populates a documents template, and updates a mkdocs configuration. This can be leveraged across a multitude of projects to drive many use cases. It only makes sense to rip out the code and turn it into a proper piece of software. This is the start of the document archiver, [DocHive](https://github.com/boldware/dochive). publish_date : 2021-11-17 filename : 2021-11-17-introducing-dochive --- Figure 3 - Proposed options for digest Description of proposed command options: --template - jinja template to render the digest doc --limit - the most recent N docs to read from --input-dir - the doc directory to search --output-file - filename of the new digest doc Expansion Packaging Python packaging will be the next step in DocHive's evolution once the general commands are fully supported and documented. Packaging is more or less the bow on top. It's not necessary, but it gives the tool that official \ud83e\udd0f GitHub Action Static document sites for software projects typically have a section that would be considered an archive. Release Notes Root Cause Analysis Devlogs Proof of Concepts Assuming these projects utilize GitHub Actions for CI/CD, it would be great to create an action that: Adds new documents using Issue bodies as input Creates a digest file using recent files in a directory This could then be incorporated in the automation pipeline, making documentation a bit easier to approach. The main advantage here is simplifying the use of DocHive within Actions. The developer would no longer need to install python nor the dependencies necessary to run DocHive in their job. Just add the action and populate the options. Conclusion Updates on DocHive will mainly live on the Project Site . Comments Continue the discussion here !","title":"Introducing, DocHive!"},{"location":"blog/2021-11-17-introducing-dochive.html#introducing-dochive","text":"Posted by Jason Bolden on Nov 17, 2021 After reusing the code powering this blog across multiple projects, a definite pattern has come to light. At its core, the profile_builder module takes a list of key/value pairs as input, populates a documents template, and updates a mkdocs configuration. This can be leveraged across a multitude of projects to drive many use cases. It only makes sense to rip out the code and turn it into a proper piece of software. This is the start of the document archiver, DocHive .","title":"Introducing, DocHive!"},{"location":"blog/2021-11-17-introducing-dochive.html#conceptual","text":"DocHive is intended to be a simple utility. Given three inputs, DocHive will output a new document for your archive and update the nav section of a mkdocs config file. For now, DocHive will be built to compliment mkdocs and leverages Jinja for templates. DocHive is not a cookiecutter replacement. Figure 1 - DocHive Conceptual","title":"Conceptual"},{"location":"blog/2021-11-17-introducing-dochive.html#general-usage","text":"Continuing with the theme of simplicity, DocHive will have 2 commands initially.","title":"General Usage"},{"location":"blog/2021-11-17-introducing-dochive.html#add","text":"The add command creates a new document and adds it to the archive. The following options describe ways to manipulate the generated document and how mkdocs should navigate to it. Figure 2 - Proposed options for add Description of proposed command options: --template - jinja template to render the new doc --config-file - yaml file containing key/value pairs to populate the template --config - inline key/value pair to input on the commandline --nav - navigation path in the mkdocs config to add the new doc (supports date format codes) --timestamp - timestamp used to populate date format codes (defaults to time of execution) --output-dir - directory to output generated doc --mkdocs-file - mkdocs yaml file to update with new doc addition","title":"add"},{"location":"blog/2021-11-17-introducing-dochive.html#digest","text":"The digest command reads metadata from the last N documents in the archive and populates a template file using them. The immediate use case this addresses is, for example, a \"Recent Posts\" section of a blog page. The digest command can be used to populate cards for the 3 most recent posts to the blog. The metadata is simply yaml formatted data at the top of the document in a --- blocked section. For example: 1 2 3 4 5 6 --- title : Introducing, DocHive! description : After reusing the code powering this blog across multiple projects, a definite pattern has come to light. At its core, the [`profile_builder`](https://jbold569.github.io/profile/blog/2021-07-17-automating-docs-as-code.html) module takes a list of key/value pairs as input, populates a documents template, and updates a mkdocs configuration. This can be leveraged across a multitude of projects to drive many use cases. It only makes sense to rip out the code and turn it into a proper piece of software. This is the start of the document archiver, [DocHive](https://github.com/boldware/dochive). publish_date : 2021-11-17 filename : 2021-11-17-introducing-dochive --- Figure 3 - Proposed options for digest Description of proposed command options: --template - jinja template to render the digest doc --limit - the most recent N docs to read from --input-dir - the doc directory to search --output-file - filename of the new digest doc","title":"digest"},{"location":"blog/2021-11-17-introducing-dochive.html#expansion","text":"","title":"Expansion"},{"location":"blog/2021-11-17-introducing-dochive.html#packaging","text":"Python packaging will be the next step in DocHive's evolution once the general commands are fully supported and documented. Packaging is more or less the bow on top. It's not necessary, but it gives the tool that official \ud83e\udd0f","title":"Packaging"},{"location":"blog/2021-11-17-introducing-dochive.html#github-action","text":"Static document sites for software projects typically have a section that would be considered an archive. Release Notes Root Cause Analysis Devlogs Proof of Concepts Assuming these projects utilize GitHub Actions for CI/CD, it would be great to create an action that: Adds new documents using Issue bodies as input Creates a digest file using recent files in a directory This could then be incorporated in the automation pipeline, making documentation a bit easier to approach. The main advantage here is simplifying the use of DocHive within Actions. The developer would no longer need to install python nor the dependencies necessary to run DocHive in their job. Just add the action and populate the options.","title":"GitHub Action"},{"location":"blog/2021-11-17-introducing-dochive.html#conclusion","text":"Updates on DocHive will mainly live on the Project Site .","title":"Conclusion"},{"location":"blog/readme.html","text":"Blogs Welcome to my collection of thoughts, lessons learned, and experiments. I've gather them here not only for my own record keeping, but also in the hopes that someone finds them beneficial and learns something new. They're organized under 3 main categories. Tech Blogs Flickr Inge Knoff Tech blogs are \"rubber meets pavement\" write ups on applied technology. They start with a problem, propose a solution, and walk through the steps. Some may also have an accompanying GitHub repo for easier setup. Idea Blogs Flickr Shelby L. Bell These blogs are more or less engineering journal entries. Inspiration could come at any moment, so why not have a nice place to store it. Idea blogs may spawn off into future projects or tech blogs. The main goal is to capture fleeting thoughts that could turn into fruitful endeavors. Insight Blogs Flickr Carly Hagins Insights are bits of corporate wisdom that I've picked up over the year. IT is moves at a fast pace, and with that comes buzzwords, hard and fast culture shifts, and a lot of bumps and bruises. These types of blogs are my way of sharing experiences I've learned from personally and potentially gain back some sanity by resonating with others. Recent Posts Introducing, DocHive! November 17, 2021 After reusing the code powering this blog across multiple projects, a definite pattern has come to light. At its core, the profile_builder module takes a list of key/value pairs as input, populates a docum... Read More Hello, GitHub Discussions! Goodbye, Disqus... November 02, 2021 In short, I don't like the free version of Disqus. Ads serve a purpose in this world, and they often allow us to enjoy a lot of valuable content at the cost of a portion of our attention. However, I d... Read More Agent Based Logging July 25, 2021 Whether you're working in site reliability, business intelligence, or security, one thing rings true. Logs are king. They are a window into your operations, providing insights into access, change, per... Read More","title":"Content"},{"location":"blog/readme.html#blogs","text":"Welcome to my collection of thoughts, lessons learned, and experiments. I've gather them here not only for my own record keeping, but also in the hopes that someone finds them beneficial and learns something new. They're organized under 3 main categories.","title":"Blogs"},{"location":"blog/readme.html#tech-blogs","text":"Flickr Inge Knoff Tech blogs are \"rubber meets pavement\" write ups on applied technology. They start with a problem, propose a solution, and walk through the steps. Some may also have an accompanying GitHub repo for easier setup.","title":"Tech Blogs"},{"location":"blog/readme.html#idea-blogs","text":"Flickr Shelby L. Bell These blogs are more or less engineering journal entries. Inspiration could come at any moment, so why not have a nice place to store it. Idea blogs may spawn off into future projects or tech blogs. The main goal is to capture fleeting thoughts that could turn into fruitful endeavors.","title":"Idea Blogs"},{"location":"blog/readme.html#insight-blogs","text":"Flickr Carly Hagins Insights are bits of corporate wisdom that I've picked up over the year. IT is moves at a fast pace, and with that comes buzzwords, hard and fast culture shifts, and a lot of bumps and bruises. These types of blogs are my way of sharing experiences I've learned from personally and potentially gain back some sanity by resonating with others.","title":"Insight Blogs"},{"location":"blog/readme.html#recent-posts","text":"","title":"Recent Posts"},{"location":"events/index.html","text":"Events","title":"Events"},{"location":"events/index.html#events","text":"","title":"Events"},{"location":"projects/index.html","text":"Projects","title":"Projects"},{"location":"projects/index.html#projects","text":"","title":"Projects"}]}